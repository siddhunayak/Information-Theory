{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVbUlJbZm0Wj"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Install required stable packages (run once)\n",
        "!pip install -q torch pandas numpy scikit-learn scipy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Imports, seed, and device\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from scipy.stats import entropy\n",
        "from collections import Counter\n",
        "\n",
        "# Reproducibility\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdkuR2hwm-CQ",
        "outputId": "4bf8d7fc-6bb4-4393-91b4-c7d679265a18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Load and basic cleaning\n",
        "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n",
        "columns = [\n",
        "    'age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status',\n",
        "    'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss',\n",
        "    'hours-per-week', 'native-country', 'income'\n",
        "]\n",
        "\n",
        "data = pd.read_csv(url, header=None, names=columns, na_values='?', skipinitialspace=True)\n",
        "print(\"Original dataset shape:\", data.shape)\n",
        "\n",
        "# Drop missing rows\n",
        "data.dropna(inplace=True)\n",
        "data.reset_index(drop=True, inplace=True)\n",
        "print(\"After cleaning, dataset shape:\", data.shape)\n",
        "\n",
        "# Define sensitive attribute and target\n",
        "SENSITIVE_ATTR = 'sex'\n",
        "TARGET = 'income'\n",
        "\n",
        "# Binarize\n",
        "data[SENSITIVE_ATTR] = data[SENSITIVE_ATTR].apply(lambda x: 1 if x == 'Male' else 0)\n",
        "data[TARGET] = data[TARGET].apply(lambda x: 1 if x == '>50K' else 0)\n",
        "\n",
        "features = data.drop(columns=[TARGET, SENSITIVE_ATTR])\n",
        "sensitive_attrs = data[SENSITIVE_ATTR]\n",
        "labels = data[TARGET]\n",
        "\n",
        "print(\"Features shape:\", features.shape)\n",
        "print(\"Sensitive attribute counts:\\n\", sensitive_attrs.value_counts())\n",
        "print(\"Target counts:\\n\", labels.value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-rR1uH7oxUb",
        "outputId": "25533115-1adb-46ff-fcce-eb8e754bd8de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original dataset shape: (32561, 15)\n",
            "After cleaning, dataset shape: (30162, 15)\n",
            "Features shape: (30162, 13)\n",
            "Sensitive attribute counts:\n",
            " sex\n",
            "1    20380\n",
            "0     9782\n",
            "Name: count, dtype: int64\n",
            "Target counts:\n",
            " income\n",
            "0    22654\n",
            "1     7508\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Split and preprocessing (fit on train only)\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "categorical_features = features.select_dtypes(include=['object']).columns.tolist()\n",
        "numerical_features = features.select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "print(\"Categorical features:\", categorical_features)\n",
        "print(\"Numerical features:\", numerical_features)\n",
        "\n",
        "numerical_transformer = StandardScaler()\n",
        "\n",
        "# âœ… Updated for sklearn >= 1.4\n",
        "categorical_transformer = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Split (stratify by label)\n",
        "X_train, X_test, y_train, y_test, A_train, A_test = train_test_split(\n",
        "    features, labels, sensitive_attrs, test_size=0.2, random_state=RANDOM_SEED, stratify=labels\n",
        ")\n",
        "\n",
        "# Fit preprocessor on train and transform both\n",
        "pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
        "X_train_processed = pipeline.fit_transform(X_train)\n",
        "X_test_processed = pipeline.transform(X_test)\n",
        "\n",
        "print(\"Processed X_train shape:\", X_train_processed.shape)\n",
        "print(\"Processed X_test shape:\", X_test_processed.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vp5VGxYcoyAw",
        "outputId": "f0e83452-451a-4158-c57e-8bab7528f3a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Categorical features: ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'native-country']\n",
            "Numerical features: ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
            "Processed X_train shape: (24129, 102)\n",
            "Processed X_test shape: (6033, 102)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Torch Dataset and DataLoaders\n",
        "class AdultDataset(Dataset):\n",
        "    def __init__(self, features, labels, sensitive_attrs):\n",
        "        self.features = torch.tensor(features, dtype=torch.float32)\n",
        "        self.labels = torch.tensor(labels.values, dtype=torch.float32).unsqueeze(1)\n",
        "        self.sensitive_attrs = torch.tensor(sensitive_attrs.values, dtype=torch.int64)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.labels[idx], self.sensitive_attrs[idx]\n",
        "\n",
        "BATCH_SIZE = 256\n",
        "train_dataset = AdultDataset(X_train_processed, y_train, A_train)\n",
        "test_dataset = AdultDataset(X_test_processed, y_test, A_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(\"Train batches:\", len(train_loader), \"Test batches:\", len(test_loader))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_oWDIlUWpG5T",
        "outputId": "52df94ea-6bf8-4b42-cf4f-288ed8f2ff1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train batches: 95 Test batches: 24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: MLP model\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(MLP, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(64, 1)  # output is a logit\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "input_dim = X_train_processed.shape[1]\n",
        "print(\"Input dim:\", input_dim)\n",
        "# instantiate example (you will instantiate again before training to ensure fresh weights)\n",
        "_example_model = MLP(input_dim)\n",
        "print(_example_model)\n",
        "del _example_model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQjT_eJSrNpY",
        "outputId": "276ee5bf-6bb5-4480-9ef7-361e7a684984"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input dim: 102\n",
            "MLP(\n",
            "  (layers): Sequential(\n",
            "    (0): Linear(in_features=102, out_features=128, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.2, inplace=False)\n",
            "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Dropout(p=0.2, inplace=False)\n",
            "    (6): Linear(in_features=64, out_features=1, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: PID approximation function + PIDLoss module\n",
        "def calculate_pid_components(logits, labels, sensitive_attrs):\n",
        "    \"\"\"\n",
        "    Approximate PID-like components using mutual information as a proxy.\n",
        "    logits, labels, sensitive_attrs are torch tensors (possibly on GPU).\n",
        "    This function moves them to CPU and returns simple approximations.\n",
        "    \"\"\"\n",
        "    # Safe flatten + detach + cpu -> numpy\n",
        "    preds = (torch.sigmoid(logits).view(-1) > 0.5).long().detach().cpu().numpy()\n",
        "    labels_np = labels.view(-1).long().detach().cpu().numpy()\n",
        "    sensitive_np = sensitive_attrs.view(-1).long().detach().cpu().numpy()\n",
        "\n",
        "    # mutual information estimates (labels vs sensitive) and (preds vs sensitive)\n",
        "    # mutual_info_classif expects (n_samples, n_features) for X\n",
        "    try:\n",
        "        mi_y_s = float(mutual_info_classif(labels_np.reshape(-1, 1), sensitive_np, discrete_features=True)[0])\n",
        "        mi_pred_s = float(mutual_info_classif(preds.reshape(-1, 1), sensitive_np, discrete_features=True)[0])\n",
        "    except Exception:\n",
        "        mi_y_s = 0.0\n",
        "        mi_pred_s = 0.0\n",
        "\n",
        "    # A simple proxy:\n",
        "    # - 'red_YA' (redundant) ~ mi_y_s\n",
        "    # - 'unq_A_if_Y' (unique from A about Y_hat given Y) ~ max(mi_pred_s - mi_y_s, 0)\n",
        "    # - 'syn_YA' left as 0 (not estimated here)\n",
        "    unq = max(mi_pred_s - mi_y_s, 0.0)\n",
        "\n",
        "    return {\n",
        "        'unq_A_if_Y': unq,\n",
        "        'red_YA': mi_y_s,\n",
        "        'syn_YA': 0.0\n",
        "    }\n",
        "\n",
        "class PIDLoss(nn.Module):\n",
        "    def __init__(self, component_to_penalize='unq_A_if_Y'):\n",
        "        super(PIDLoss, self).__init__()\n",
        "        self.component_to_penalize = component_to_penalize\n",
        "\n",
        "    def forward(self, logits, labels, sensitive_attrs):\n",
        "        pid_components = calculate_pid_components(logits, labels, sensitive_attrs)\n",
        "        loss_value = float(pid_components.get(self.component_to_penalize, 0.0))\n",
        "        # Return as a CPU scalar tensor (no gradient)\n",
        "        return torch.tensor(loss_value, dtype=torch.float32, requires_grad=False)\n"
      ],
      "metadata": {
        "id": "6UIB9Wd6rQx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Training and evaluation helper functions\n",
        "def train_model(model, train_loader, criterion, pid_regularizer, optimizer, epochs, lam, device):\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_bce_loss = 0.0\n",
        "        total_pid_loss = 0.0\n",
        "        total_samples = 0\n",
        "\n",
        "        for features, labels, sensitive_attrs in train_loader:\n",
        "            features = features.to(device)\n",
        "            labels = labels.to(device)\n",
        "            sensitive_attrs = sensitive_attrs.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(features)  # shape (batch, 1)\n",
        "            bce_loss = criterion(logits, labels)\n",
        "\n",
        "            # pid_regularizer returns a CPU tensor scalar; move to device for arithmetic\n",
        "            pid_loss = pid_regularizer(logits, labels, sensitive_attrs).to(device)\n",
        "            loss = bce_loss + lam * pid_loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            batch_size = features.size(0)\n",
        "            total_bce_loss += bce_loss.item() * batch_size\n",
        "            total_pid_loss += pid_loss.item() * batch_size\n",
        "            total_samples += batch_size\n",
        "\n",
        "        avg_bce = total_bce_loss / total_samples\n",
        "        avg_pid = total_pid_loss / total_samples\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | BCE: {avg_bce:.4f} | PID({pid_regularizer.component_to_penalize}): {avg_pid:.6f}\")\n",
        "\n",
        "def evaluate_model(model, dataloader, device):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_trues = []\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch, _ in dataloader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            logits = model(X_batch)\n",
        "            preds = (torch.sigmoid(logits) > 0.5).long().cpu().numpy().flatten()\n",
        "            all_preds.extend(preds)\n",
        "            all_trues.extend(y_batch.cpu().numpy().flatten().astype(int))\n",
        "\n",
        "    acc = accuracy_score(all_trues, all_preds)\n",
        "    f1 = f1_score(all_trues, all_preds)\n",
        "    print(f\"Eval -> Accuracy: {acc:.4f} | F1: {f1:.4f}\")\n",
        "    return np.array(all_preds), np.array(all_trues)\n"
      ],
      "metadata": {
        "id": "-sxwMS8OrTGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Train baseline (lambda=0) and PID-regularized (lambda>0) models\n",
        "LEARNING_RATE = 1e-3\n",
        "EPOCHS = 5\n",
        "LAMBDA = 1.0\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "pid_regularizer = PIDLoss(component_to_penalize='unq_A_if_Y')\n",
        "\n",
        "# Baseline model (no PID penalty)\n",
        "print(\"\\n--- Baseline training (lambda=0) ---\")\n",
        "baseline_model = MLP(input_dim)\n",
        "baseline_optimizer = optim.Adam(baseline_model.parameters(), lr=LEARNING_RATE)\n",
        "train_model(baseline_model, train_loader, criterion, pid_regularizer, baseline_optimizer, EPOCHS, 0.0, device)\n",
        "baseline_preds, baseline_trues = evaluate_model(baseline_model, test_loader, device)\n",
        "\n",
        "# PID-regularized model\n",
        "print(\"\\n--- PID-regularized training (lambda=1.0) ---\")\n",
        "pid_model = MLP(input_dim)\n",
        "pid_optimizer = optim.Adam(pid_model.parameters(), lr=LEARNING_RATE)\n",
        "train_model(pid_model, train_loader, criterion, pid_regularizer, pid_optimizer, EPOCHS, LAMBDA, device)\n",
        "pid_preds, pid_trues = evaluate_model(pid_model, test_loader, device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOFrCvBMrasG",
        "outputId": "54f8aece-d5e1-4329-c938-ef52cb248e61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Baseline training (lambda=0) ---\n",
            "Epoch 1/5 | BCE: 0.4303 | PID(unq_A_if_Y): 0.006571\n",
            "Epoch 2/5 | BCE: 0.3252 | PID(unq_A_if_Y): 0.005031\n",
            "Epoch 3/5 | BCE: 0.3199 | PID(unq_A_if_Y): 0.003216\n",
            "Epoch 4/5 | BCE: 0.3153 | PID(unq_A_if_Y): 0.004446\n",
            "Epoch 5/5 | BCE: 0.3136 | PID(unq_A_if_Y): 0.003019\n",
            "Eval -> Accuracy: 0.8497 | F1: 0.6739\n",
            "\n",
            "--- PID-regularized training (lambda=1.0) ---\n",
            "Epoch 1/5 | BCE: 0.4280 | PID(unq_A_if_Y): 0.006407\n",
            "Epoch 2/5 | BCE: 0.3267 | PID(unq_A_if_Y): 0.004461\n",
            "Epoch 3/5 | BCE: 0.3187 | PID(unq_A_if_Y): 0.003796\n",
            "Epoch 4/5 | BCE: 0.3160 | PID(unq_A_if_Y): 0.003869\n",
            "Epoch 5/5 | BCE: 0.3117 | PID(unq_A_if_Y): 0.004073\n",
            "Eval -> Accuracy: 0.8536 | F1: 0.6790\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: Fairness metrics on test set\n",
        "def get_preds_trues_attrs(model, dataloader, device):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    preds, trues, attrs = [], [], []\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch, a_batch in dataloader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            logits = model(X_batch)\n",
        "            batch_preds = (torch.sigmoid(logits) > 0.5).long().cpu().numpy().flatten()\n",
        "            preds.extend(batch_preds)\n",
        "            trues.extend(y_batch.cpu().numpy().flatten().astype(int))\n",
        "            attrs.extend(a_batch.cpu().numpy().flatten().astype(int))\n",
        "    return np.array(preds), np.array(trues), np.array(attrs)\n",
        "\n",
        "def demographic_parity_difference(preds, attrs):\n",
        "    # P(Yhat=1 | A=1) - P(Yhat=1 | A=0)\n",
        "    if (attrs==1).sum() == 0 or (attrs==0).sum() == 0:\n",
        "        return 0.0\n",
        "    p1 = preds[attrs==1].mean()\n",
        "    p0 = preds[attrs==0].mean()\n",
        "    return float(p1 - p0)\n",
        "\n",
        "def equal_opportunity_difference(preds, trues, attrs):\n",
        "    # TPR difference: TPR(A=1) - TPR(A=0)\n",
        "    mask_pos = trues == 1\n",
        "    if mask_pos.sum() == 0:\n",
        "        return 0.0\n",
        "    denom1 = ((mask_pos) & (attrs==1)).sum()\n",
        "    denom0 = ((mask_pos) & (attrs==0)).sum()\n",
        "    tpr1 = ((preds==1) & (trues==1) & (attrs==1)).sum() / denom1 if denom1 > 0 else 0.0\n",
        "    tpr0 = ((preds==1) & (trues==1) & (attrs==0)).sum() / denom0 if denom0 > 0 else 0.0\n",
        "    return float(tpr1 - tpr0)\n",
        "\n",
        "# Baseline fairness\n",
        "b_preds, b_trues, b_attrs = get_preds_trues_attrs(baseline_model, test_loader, device)\n",
        "dp_diff_baseline = demographic_parity_difference(b_preds, b_attrs)\n",
        "eo_diff_baseline = equal_opportunity_difference(b_preds, b_trues, b_attrs)\n",
        "print(\"Baseline fairness:\")\n",
        "print(\" Demographic Parity Difference (A=1 - A=0):\", dp_diff_baseline)\n",
        "print(\" Equal Opportunity Difference (TPR diff):\", eo_diff_baseline)\n",
        "\n",
        "# PID model fairness\n",
        "p_preds, p_trues, p_attrs = get_preds_trues_attrs(pid_model, test_loader, device)\n",
        "dp_diff_pid = demographic_parity_difference(p_preds, p_attrs)\n",
        "eo_diff_pid = equal_opportunity_difference(p_preds, p_trues, p_attrs)\n",
        "print(\"\\nPID-regularized model fairness:\")\n",
        "print(\" Demographic Parity Difference (A=1 - A=0):\", dp_diff_pid)\n",
        "print(\" Equal Opportunity Difference (TPR diff):\", eo_diff_pid)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_mc8dzyrdYn",
        "outputId": "ad756fbd-c0d8-4b66-f2fc-9b52ea98b3cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline fairness:\n",
            " Demographic Parity Difference (A=1 - A=0): 0.1653189031890319\n",
            " Equal Opportunity Difference (TPR diff): 0.033300307309946486\n",
            "\n",
            "PID-regularized model fairness:\n",
            " Demographic Parity Difference (A=1 - A=0): 0.1639718897188972\n",
            " Equal Opportunity Difference (TPR diff): 0.05111084988832726\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KojYlIK0rn8w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}